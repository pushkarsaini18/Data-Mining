{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e4ac78",
   "metadata": {},
   "source": [
    "#### Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d14805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                  #Requests will allow you to send HTTP/1.1 requests using Python\n",
    "import pandas as pd              #Python package providing fast, flexible, and expressive data structures\n",
    "from bs4 import BeautifulSoup    #for web scraping purposes to pull the data out of HTML and XML files\n",
    "from textblob import TextBlob    #for processing textual data\n",
    "import textstat                  # to calculate statistics from. # text\n",
    "import openpyxl                 #a Python library for reading and writing Excel\n",
    "import string                  #contains a number of functions to process standard Python strings,\n",
    "import spacy                  #for “Industrial strength NLP in Python\n",
    "import re                      #provides full support for Perl-like regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe0a75",
   "metadata": {},
   "source": [
    "#### Import NLTK for tokenizing, Tagging, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71a74369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                 #toolkit build for working with NLP in Python eg: tokenizing, Tagging, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a7b9c",
   "metadata": {},
   "source": [
    "#### Positive & Negative words dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5f0ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"G:/Data science/assignment/text minning(done)/negative-words.txt\",\"r\",encoding = \"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2dee0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"G:/Data science/assignment/text minning(done)/positive-words.txt\",\"r\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290842f",
   "metadata": {},
   "source": [
    "#### Python program to fetch link and perform required operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "791120a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "data=[]\n",
    "\n",
    "def text(): \n",
    "\n",
    "    ######## for fetching data from given input excel sheet ########\n",
    "    wb = openpyxl.load_workbook('Input.xlsx')  \n",
    "    ws = wb['Sheet1']\n",
    "    \n",
    "    ####### fetch the url from sheet into url variable ############ \n",
    "    for i in range (2 , 172):\n",
    "        url = (ws.cell(row=i, column=2).value)\n",
    "    \n",
    "    #We need to pass argument called Headers by passing \"User-Agent\" to the request to bypass the mod-security error.\n",
    "\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}     \n",
    "        page = requests.get(url, headers=headers)\n",
    "    \n",
    "    # apply BeautifulSoup to fetch only html parser \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "    # fetch title from link\n",
    "        title = soup.find('h1',class_=\"entry-title\").text.replace('\\n',\" \")\n",
    "    \n",
    "    #fetch content from link and remove unwanted header & punctuation\n",
    "        content = soup.findAll(attrs={'class':'td-post-content'})    \n",
    "        content = content[0].text.replace('\\n',\" \")    \n",
    "        content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    #tokenize the data \n",
    "        text_tokens = word_tokenize(content)\n",
    "        \n",
    "    \n",
    "    #remove stopwords\n",
    "        my_stop_words = stopwords.words('english')\n",
    "        no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "    \n",
    "    #count positive score usnig positive dictionary \n",
    "        pos_count = \" \".join ([w for w in no_stop_tokens if w in poswords])   \n",
    "        pos_count=pos_count.split(\" \")  \n",
    "        Positive_score=len(pos_count)\n",
    "    \n",
    "    #count negative score usnig negative dictionary\n",
    "        neg_count = \" \".join ([w for w in no_stop_tokens if w in negwords])    \n",
    "        neg_count=neg_count.split(\" \")    \n",
    "        Negative_score=len(neg_count)\n",
    "        \n",
    "    #join filter data after removing stpowords  \n",
    "        filter_content = ' '.join(no_stop_tokens)\n",
    "        \n",
    "    #words count \n",
    "        Word_Count=len(content)\n",
    "        \n",
    "    #Avg_Sentence_Lenght count \n",
    "        Avg_Sentence_Lenght = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "    \n",
    "    #calculating fog index using textstat library\n",
    "        Fog_Index=(textstat.gunning_fog(content))\n",
    "    \n",
    "    #Avg_Number_of_Words_Per_Sentence count\n",
    "        Avg_Number_of_Words_Per_Sentence = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "        Avg_Number_of_Words_Per_Sentence=(sum(Avg_Number_of_Words_Per_Sentence)/len(Avg_Number_of_Words_Per_Sentence))\n",
    "    \n",
    "        Word_Count=len(content)\n",
    "\n",
    "        \n",
    "    #function to calculate Complex_Words consedring word not ending from \"ed\" or \"es\"\n",
    "        def syllablecount(word):\n",
    "            coun = 0\n",
    "            vowels = \"AEIOUYaeiouy\"\n",
    "            if word[0] in vowels:\n",
    "                coun = coun + 1\n",
    "            for index in range(1, len(word)): \n",
    "                    if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                        coun = coun + 1\n",
    "                        if word.endswith(\"es\"or \"ed\"):\n",
    "                            coun = coun - 1\n",
    "            if coun == 0:\n",
    "                coun = coun + 1\n",
    "            return coun\n",
    "        Complex_Words = syllablecount(content)\n",
    "\n",
    "    \n",
    "    #function to calculate proper noun in article with help of tagging from nltk lib\n",
    "        def ProperNounExtractor(text):\n",
    "            cou = 0\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                for (word, tag) in tagged:\n",
    "                    if tag == 'PRP': # If the word is a proper noun\n",
    "                        cou = cou + 1 \n",
    "        \n",
    "            return(cou) \n",
    "        Personal_Pronouns=ProperNounExtractor(content)\n",
    "    \n",
    "\n",
    "    #function for sentiment analysis\n",
    "        def sentiment_analysis(text):\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            return (sentiment.polarity)\n",
    "    \n",
    "        polarity=sentiment_analysis(content)\n",
    "  \n",
    "        def sentiment_analysis(text):\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            return (sentiment.subjectivity)\n",
    "    \n",
    "        subjectivity=sentiment_analysis(content)\n",
    "        \n",
    "        \n",
    "    #method to count average syllable count in words\n",
    "        word=content.replace(' ','')\n",
    "        syllable_count = 0\n",
    "        for w in word:\n",
    "            \n",
    "            if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "                syllable_count=syllable_count+1\n",
    "\n",
    "        Syllable_Per_Word=(syllable_count/len(content.split()))\n",
    "        \n",
    "    # calculate average word lenght \n",
    "        Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "        \n",
    "    # calculate % of complex word\n",
    "        Percentage_of_Complex_Word = Complex_Words / Word_Count * 100\n",
    "    \n",
    "        data.insert(i,[url,Positive_score, Negative_score, polarity,subjectivity, Avg_Sentence_Lenght,Percentage_of_Complex_Word,Fog_Index, Avg_Number_of_Words_Per_Sentence , Complex_Words, Word_Count,Syllable_Per_Word, Personal_Pronouns, Average_Word_Length])\n",
    "        \n",
    "\n",
    "    \n",
    "if __name__ == '__main__' :  \n",
    "    text()\n",
    "        \n",
    "df = pd.DataFrame(data,columns=['url','Positive_score','Negative_score','polarity','subjectivity', 'Avg_Sentence_Lenght','Percentage_of_Complex_Word', 'Fog_Index', 'Avg_Number_of_Words_Per_Sentence' , 'Complex_Words', 'Word_Count', 'Syllable_Per_Word','Personal_Pronouns', 'Average_Word_Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7e215120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Avg_Sentence_Lenght</th>\n",
       "      <th>Percentage_of_Complex_Word</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Avg_Number_of_Words_Per_Sentence</th>\n",
       "      <th>Complex_Words</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Syllable_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0.143040</td>\n",
       "      <td>0.478514</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>29.160967</td>\n",
       "      <td>289.41</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1279</td>\n",
       "      <td>4386</td>\n",
       "      <td>2.109551</td>\n",
       "      <td>12</td>\n",
       "      <td>5.158708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.177208</td>\n",
       "      <td>0.493776</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>30.125313</td>\n",
       "      <td>258.75</td>\n",
       "      <td>632.0</td>\n",
       "      <td>1202</td>\n",
       "      <td>3990</td>\n",
       "      <td>2.118671</td>\n",
       "      <td>18</td>\n",
       "      <td>5.311709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "      <td>0.141910</td>\n",
       "      <td>0.539460</td>\n",
       "      <td>9654.0</td>\n",
       "      <td>29.886562</td>\n",
       "      <td>727.01</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>3425</td>\n",
       "      <td>11460</td>\n",
       "      <td>2.148559</td>\n",
       "      <td>44</td>\n",
       "      <td>5.351441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077056</td>\n",
       "      <td>0.458329</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>30.182482</td>\n",
       "      <td>179.39</td>\n",
       "      <td>433.0</td>\n",
       "      <td>827</td>\n",
       "      <td>2740</td>\n",
       "      <td>2.166282</td>\n",
       "      <td>6</td>\n",
       "      <td>5.325635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.471385</td>\n",
       "      <td>6191.0</td>\n",
       "      <td>30.504338</td>\n",
       "      <td>493.45</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>2250</td>\n",
       "      <td>7376</td>\n",
       "      <td>2.085925</td>\n",
       "      <td>43</td>\n",
       "      <td>5.066285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>0.420271</td>\n",
       "      <td>7700.0</td>\n",
       "      <td>30.315858</td>\n",
       "      <td>609.48</td>\n",
       "      <td>1514.0</td>\n",
       "      <td>2793</td>\n",
       "      <td>9213</td>\n",
       "      <td>2.058785</td>\n",
       "      <td>32</td>\n",
       "      <td>5.085865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0.102483</td>\n",
       "      <td>0.481812</td>\n",
       "      <td>5254.0</td>\n",
       "      <td>30.347938</td>\n",
       "      <td>384.28</td>\n",
       "      <td>949.0</td>\n",
       "      <td>1884</td>\n",
       "      <td>6208</td>\n",
       "      <td>2.163330</td>\n",
       "      <td>10</td>\n",
       "      <td>5.536354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.486082</td>\n",
       "      <td>5253.0</td>\n",
       "      <td>30.990672</td>\n",
       "      <td>390.10</td>\n",
       "      <td>963.0</td>\n",
       "      <td>1927</td>\n",
       "      <td>6218</td>\n",
       "      <td>2.229491</td>\n",
       "      <td>17</td>\n",
       "      <td>5.454829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057445</td>\n",
       "      <td>0.480637</td>\n",
       "      <td>2878.0</td>\n",
       "      <td>30.420992</td>\n",
       "      <td>238.50</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1055</td>\n",
       "      <td>3468</td>\n",
       "      <td>2.003413</td>\n",
       "      <td>20</td>\n",
       "      <td>4.911263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>0.066522</td>\n",
       "      <td>0.410040</td>\n",
       "      <td>3948.0</td>\n",
       "      <td>29.090523</td>\n",
       "      <td>308.05</td>\n",
       "      <td>760.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4706</td>\n",
       "      <td>2.027632</td>\n",
       "      <td>7</td>\n",
       "      <td>5.194737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  Positive_score  \\\n",
       "0    https://insights.blackcoffer.com/how-is-login-...              16   \n",
       "1    https://insights.blackcoffer.com/how-does-ai-h...              27   \n",
       "2    https://insights.blackcoffer.com/ai-and-its-im...              76   \n",
       "3    https://insights.blackcoffer.com/how-do-deep-l...              14   \n",
       "4    https://insights.blackcoffer.com/how-artificia...              51   \n",
       "..                                                 ...             ...   \n",
       "165  https://insights.blackcoffer.com/role-big-data...              41   \n",
       "166  https://insights.blackcoffer.com/sales-forecas...              36   \n",
       "167  https://insights.blackcoffer.com/detect-data-e...              17   \n",
       "168  https://insights.blackcoffer.com/data-exfiltra...               2   \n",
       "169  https://insights.blackcoffer.com/impacts-of-co...              25   \n",
       "\n",
       "     Negative_score  polarity  subjectivity  Avg_Sentence_Lenght  \\\n",
       "0                 9  0.143040      0.478514               3673.0   \n",
       "1                 3  0.177208      0.493776               3357.0   \n",
       "2                20  0.141910      0.539460               9654.0   \n",
       "3                 1  0.077056      0.458329               2306.0   \n",
       "4                13  0.063581      0.471385               6191.0   \n",
       "..              ...       ...           ...                  ...   \n",
       "165              18  0.130195      0.420271               7700.0   \n",
       "166              19  0.102483      0.481812               5254.0   \n",
       "167              40  0.050360      0.486082               5253.0   \n",
       "168               7  0.057445      0.480637               2878.0   \n",
       "169              13  0.066522      0.410040               3948.0   \n",
       "\n",
       "     Percentage_of_Complex_Word  Fog_Index  Avg_Number_of_Words_Per_Sentence  \\\n",
       "0                     29.160967     289.41                             712.0   \n",
       "1                     30.125313     258.75                             632.0   \n",
       "2                     29.886562     727.01                            1804.0   \n",
       "3                     30.182482     179.39                             433.0   \n",
       "4                     30.504338     493.45                            1222.0   \n",
       "..                          ...        ...                               ...   \n",
       "165                   30.315858     609.48                            1514.0   \n",
       "166                   30.347938     384.28                             949.0   \n",
       "167                   30.990672     390.10                             963.0   \n",
       "168                   30.420992     238.50                             586.0   \n",
       "169                   29.090523     308.05                             760.0   \n",
       "\n",
       "     Complex_Words  Word_Count  Syllable_Per_Word  Personal_Pronouns  \\\n",
       "0             1279        4386           2.109551                 12   \n",
       "1             1202        3990           2.118671                 18   \n",
       "2             3425       11460           2.148559                 44   \n",
       "3              827        2740           2.166282                  6   \n",
       "4             2250        7376           2.085925                 43   \n",
       "..             ...         ...                ...                ...   \n",
       "165           2793        9213           2.058785                 32   \n",
       "166           1884        6208           2.163330                 10   \n",
       "167           1927        6218           2.229491                 17   \n",
       "168           1055        3468           2.003413                 20   \n",
       "169           1369        4706           2.027632                  7   \n",
       "\n",
       "     Average_Word_Length  \n",
       "0               5.158708  \n",
       "1               5.311709  \n",
       "2               5.351441  \n",
       "3               5.325635  \n",
       "4               5.066285  \n",
       "..                   ...  \n",
       "165             5.085865  \n",
       "166             5.536354  \n",
       "167             5.454829  \n",
       "168             4.911263  \n",
       "169             5.194737  \n",
       "\n",
       "[170 rows x 14 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('linkd.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cd006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
